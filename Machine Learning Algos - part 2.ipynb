{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  1.1  2.1  1.2  2.2  3  3.1  3.2  3.3  ...   3.8  2.19  3.9  4.6  \\\n",
      "0  3  3    4    2    1    2  2    4    3    2  ...     1     4    3    4   \n",
      "1  4  1    4    4    4    4  1    1    2    1  ...     1     2    1    1   \n",
      "2  1  4    1    1    3    3  4    4    3    4  ...     1     3    3    4   \n",
      "3  3  4    4    3    1    1  4    4    4    1  ...     3     1    3    2   \n",
      "4  3  2    3    1    2    3  4    3    1    4  ...     2     2    2    3   \n",
      "\n",
      "   2.20  2.21  2.22  2.23  1.15  1.16  \n",
      "0     4     4     1     3     4     1  \n",
      "1     4     2     2     4     4     1  \n",
      "2     1     3     3     4     2    -1  \n",
      "3     1     4     2     1     1    -1  \n",
      "4     4     2     2     2     2    -1  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/HP/Downloads/DataDNA.csv\")\n",
    "print(data.head())\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X.values, y.values, test_size=0.30, random_state=42)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Positive Class Percentage = ', 51.65984538426558, '%')\n",
      "('Negative Class Percentage = ', 48.34015461573443, '%')\n"
     ]
    }
   ],
   "source": [
    "label_pos = data[data['1.16']==1].shape[0]\n",
    "label_neg = data[data['1.16']==-1].shape[0]\n",
    "\n",
    "class_zero_pos = (float(label_pos) / float(data.shape[0]))*100 \n",
    "class_one_neg = (float(label_neg) / float(data.shape[0]))*100\n",
    "\n",
    "print(\"Positive Class Percentage = \",class_zero_pos, \"%\")\n",
    "print(\"Negative Class Percentage = \",class_one_neg, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaination:\n",
    "We are using z-score normalization over min-max normalization because we want our data to be distributed like a standard normal\n",
    "distribution. In addition, there is no class imbalance in the data set as the percentage of positive class instances is simmilar\n",
    "to negative class instances. Finally, we split train and test data set randomly becuase we want shuffling on data so that there \n",
    "is no class imbalance among the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('best_neighbor = ', {'n_neighbors': 11})\n",
      "('accuracy list', [0.7244881253349994, 0.7361714199063645, 0.7381308557751383, 0.7433075261267992, 0.7485209160560826, 0.7439509285806388, 0.7342195195506962, 0.7277230480275483, 0.721225307417331, 0.7205856529677162, 0.7192810512125717, 0.717987841612561, 0.7121386094329534, 0.7127845994417518, 0.7069277576778088, 0.7101817018621546])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finding the best parameters for KNN classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "neighbor_set = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31]\n",
    "parameter = {'n_neighbors': neighbor_set}\n",
    "knn = KNeighborsClassifier()\n",
    "clf_knn = GridSearchCV(knn,parameter,cv=5)\n",
    "clf_knn.fit(X_train,y_train)\n",
    "best_neighbor = clf_knn.best_params_\n",
    "print(\"best_neighbor = \",best_neighbor)\n",
    "\n",
    "accuracy_list=[]\n",
    "\n",
    "for i in range(0,len(neighbor_set)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor_set[i])\n",
    "    accuracy = cross_val_score(knn,X_train,y_train,scoring='accuracy')\n",
    "    accuracy_list.append( ( float(sum(accuracy)) / float(len(accuracy)) )   )\n",
    "print(\"accuracy list\", accuracy_list)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(neighbor_set,accuracy_list,marker='x')\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaination:\n",
    "We didnot evaluate on the test set directly because this would result in less estimation of error. Cross validation results in \n",
    "reduction of overfitting and better generalization to newer instances/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Parameter Set = ', {'C': 20, 'gamma': 0.01})\n"
     ]
    }
   ],
   "source": [
    "# Finding the best parameter set for SVM(RBF Kernel)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "c_values = [0.1, 0.5, 1, 2, 5,10, 20, 50]\n",
    "sigma_values = [0.01, 0.05, 0.1, 0.5, 1, 2, 5,10]\n",
    "\n",
    "parameters = {'C' : c_values , 'gamma': sigma_values}\n",
    "svc = SVC(kernel='rbf')\n",
    "clf_svc = GridSearchCV(svc,parameters,cv=5,scoring = 'accuracy')\n",
    "clf_svc.fit(X_train,y_train)\n",
    "best_parameters = clf_svc.best_params_\n",
    "print(\"Best Parameter Set = \",best_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'ROC')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the ROC Curve\n",
    "from sklearn import metrics\n",
    "\n",
    "svm = SVC(kernel='rbf',C=20,gamma=0.01)\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "plt.plot(fpr, tpr, lw=2, alpha=0.3,color='blue')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN-Score  = ', 0.7866242038216561)\n",
      "('Train Time:', 0.097, 's')\n",
      "('Test Time:', 0.207, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting  - KNN Classifier (1st Classifier)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "s1 = time.time()\n",
    "knn.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2=time.time()\n",
    "y_pred = knn.predict(X_test)\n",
    "e2 = time.time()\n",
    "knn_score = accuracy_score(y_test,y_pred)\n",
    "print(\"KNN-Score  = \",knn_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SVM-Score = ', 0.9299363057324841)\n",
      "('Train Time:', 0.51, 's')\n",
      "('Test Time:', 0.057, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting - SVM Classifier (2nd Classifier)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(kernel='rbf',C=20,gamma=0.01)\n",
    "s1 = time.time()\n",
    "svm.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2 = time.time()\n",
    "y_pred = svm.predict(X_test)\n",
    "e2 = time.time()\n",
    "svm_score = accuracy_score(y_test,y_pred)\n",
    "print(\"SVM-Score = \",svm_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Forest-Score = ', 0.945859872611465)\n",
      "('Train Time:', 0.129, 's')\n",
      "('Test Time:', 0.003, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting - Random Forest Classifier (Default Parameters) (3rd Classifier)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "s1 = time.time()\n",
    "forest.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2 = time.time()\n",
    "y_pred = forest.predict(X_test)\n",
    "e2 = time.time()\n",
    "forest_score = accuracy_score(y_test,y_pred)\n",
    "print(\"Forest-Score = \",forest_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Neural Network - Score = ', 0.9044585987261147)\n",
      "('Train Time:', 2.65, 's')\n",
      "('Test Time:', 0.004, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting - Neural Networks(Default Parameters)  (4th Classifier)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "neural_net = MLPClassifier()\n",
    "s1 = time.time()\n",
    "neural_net.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2 = time.time()\n",
    "y_pred = neural_net.predict(X_test)\n",
    "e2 = time.time()\n",
    "neural_net_score = accuracy_score(y_test,y_pred)\n",
    "print(\"Neural Network - Score = \",neural_net_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 20, 'bootstrap': False, 'criterion': 'gini', 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# Finding the best parameter set for Random Forest Classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameters = {'n_estimators': [1,3,5,7,10,12,15,20,25], 'criterion' : ('gini','entropy'), 'max_depth' : [1,2,5,10,15,None],'max_features':('auto','sqrt','log2',None),'bootstrap':(True,False)}\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "clf_forest = GridSearchCV(forest,parameters,cv=5)\n",
    "clf_forest.fit(X_train,y_train)\n",
    "best_parameters = clf_forest.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Optimal Forest-Score = ', 0.9585987261146497)\n",
      "('Train Time:', 0.212, 's')\n",
      "('Test Time:', 0.005, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting - Random Forest Classifier (Optimal Parameters) (5th Classifier)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "forest = RandomForestClassifier(criterion= 'gini',random_state=0,n_estimators=20,max_features='auto',max_depth=10,bootstrap=False)\n",
    "s1 = time.time()\n",
    "forest.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2 = time.time()\n",
    "y_pred = forest.predict(X_test)\n",
    "e2 = time.time()\n",
    "forest_score = accuracy_score(y_test,y_pred)\n",
    "print(\"Optimal Forest-Score = \",forest_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Parameters = ', {'alpha': 0.001, 'activation': 'relu', 'random_state': 1, 'solver': 'lbfgs', 'hidden_layer_sizes': 10})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'], \n",
    "    'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'alpha': 10.0 ** -np.arange(1, 10), \n",
    "    'hidden_layer_sizes': (10,10,5,10), \n",
    "    'random_state':[0,1]\n",
    "}\n",
    "neural_net = MLPClassifier()\n",
    "clf_net = GridSearchCV(neural_net, parameters, n_jobs=-1)\n",
    "clf_net.fit(X_train,y_train)\n",
    "best_parameters = clf_net.best_params_\n",
    "print(\"Best Parameters = \",best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Optimized Neural Network - Score = ', 0.910828025477707)\n",
      "('Train Time:', 0.283, 's')\n",
      "('Test Time:', 0.001, 's')\n"
     ]
    }
   ],
   "source": [
    "# Training,Testing/Predicting - Neural Network Classifier (Optimal Parameters) (6th Classifier)\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "neural_net = MLPClassifier(alpha=0.001,activation = 'relu',random_state=1,solver='lbfgs',hidden_layer_sizes=(100,))\n",
    "s1 = time.time()\n",
    "neural_net.fit(X_train,y_train)\n",
    "e1 = time.time()\n",
    "s2 = time.time()\n",
    "y_pred = neural_net.predict(X_test)\n",
    "e2 = time.time()\n",
    "neural_net_score = accuracy_score(y_test,y_pred)\n",
    "print(\"Optimized Neural Network - Score = \",neural_net_score)\n",
    "print(\"Train Time:\", round(e1 - s1, 3),\"s\")\n",
    "print(\"Test Time:\", round(e2 - s2, 3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.753820417917727)\n",
      "('Standard Deviation Accuracy = ', 0.025650254121647755)\n",
      "('Average Precision = ', 0.9512175167117545)\n",
      "('Standard Deviation Precision = ', 0.022747468158357636)\n",
      "('Average Recall = ', 0.5526222376897526)\n",
      "('Standard Deviation Recall = ', 0.03775124202819243)\n",
      "('Average F Score = ', 0.6982743480457662)\n",
      "('Standard Deviation F Score = ', 0.031064680904035125)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (1st Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            knn = KNeighborsClassifier(n_neighbors=11)\n",
    "            knn.fit(X_train,y_train)\n",
    "            y_pred = knn.predict(X_test)\n",
    "            knn_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(knn_acc)\n",
    "            knn_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(knn_pre)\n",
    "            knn_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(knn_rec)\n",
    "            knn_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(knn_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.9027156368511134)\n",
      "('Standard Deviation Accuracy = ', 0.017663920623134714)\n",
      "('Average Precision = ', 0.921350573324545)\n",
      "('Standard Deviation Precision = ', 0.02569325974288477)\n",
      "('Average Recall = ', 0.8885811059148636)\n",
      "('Standard Deviation Recall = ', 0.029989615021141235)\n",
      "('Average F Score = ', 0.9041336687154403)\n",
      "('Standard Deviation F Score = ', 0.01712186074987965)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (2nd Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            svm = SVC(kernel='rbf',C=20,gamma=0.01)\n",
    "            svm.fit(X_train,y_train)\n",
    "            y_pred = svm.predict(X_test)\n",
    "            svm_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(svm_acc)\n",
    "            svm_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(svm_pre)\n",
    "            svm_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(svm_rec)\n",
    "            svm_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(svm_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.9355813592745502)\n",
      "('Standard Deviation Accuracy = ', 0.018427829366581848)\n",
      "('Average Precision = ', 0.9699956722290802)\n",
      "('Standard Deviation Precision = ', 0.0171367448971743)\n",
      "('Average Recall = ', 0.9039348735111813)\n",
      "('Standard Deviation Recall = ', 0.027033754922418836)\n",
      "('Average F Score = ', 0.9355796023650377)\n",
      "('Standard Deviation F Score = ', 0.018031325750435142)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (3rd Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            forest = RandomForestClassifier(random_state=0)\n",
    "            forest.fit(X_train,y_train)\n",
    "            y_pred = forest.predict(X_test)\n",
    "            forest_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(forest_acc)\n",
    "            forest_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(forest_pre)\n",
    "            forest_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(forest_rec)\n",
    "            forest_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(forest_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.9001598041364567)\n",
      "('Standard Deviation Accuracy = ', 0.022365156491323753)\n",
      "('Average Precision = ', 0.9175258276713423)\n",
      "('Standard Deviation Precision = ', 0.0269006961080435)\n",
      "('Average Recall = ', 0.8868319934135459)\n",
      "('Standard Deviation Recall = ', 0.029963930349820773)\n",
      "('Average F Score = ', 0.901578729247026)\n",
      "('Standard Deviation F Score = ', 0.022451937573698212)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (4th Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            nn = MLPClassifier()\n",
    "            nn.fit(X_train,y_train)\n",
    "            y_pred = nn.predict(X_test)\n",
    "            nn_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(nn_acc)\n",
    "            nn_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(nn_pre)\n",
    "            nn_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(nn_rec)\n",
    "            nn_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(nn_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.9601745923017476)\n",
      "('Standard Deviation Accuracy = ', 0.012460986957900048)\n",
      "('Average Precision = ', 0.9734800436039744)\n",
      "('Standard Deviation Precision = ', 0.016801971056152318)\n",
      "('Average Recall = ', 0.9489097429785305)\n",
      "('Standard Deviation Recall = ', 0.01725415734883057)\n",
      "('Average F Score = ', 0.9608902951974302)\n",
      "('Standard Deviation F Score = ', 0.012211836428435017)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (5th Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            forest = RandomForestClassifier(criterion= 'gini',random_state=0,n_estimators=20,max_features='auto',max_depth=10,bootstrap=False)\n",
    "            forest.fit(X_train,y_train)\n",
    "            y_pred = forest.predict(X_test)\n",
    "            forest_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(forest_acc)\n",
    "            forest_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(forest_pre)\n",
    "            forest_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(forest_rec)\n",
    "            forest_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(forest_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Accuracy = ', 0.9095357201719575)\n",
      "('Standard Deviation Accuracy = ', 0.019672627432528768)\n",
      "('Average Precision = ', 0.920843520475066)\n",
      "('Standard Deviation Precision = ', 0.025120390635659098)\n",
      "('Average Recall = ', 0.9024825257251476)\n",
      "('Standard Deviation Recall = ', 0.024060055393685994)\n",
      "('Average F Score = ', 0.9113445782017302)\n",
      "('Standard Deviation F Score = ', 0.019936588921653658)\n"
     ]
    }
   ],
   "source": [
    "# Averages and Deviations for different Metrics (6th Classifier)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accuracy_list=[]\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "F_list=[]\n",
    "splits_list = [4,5,6,7]\n",
    "\n",
    "for i in range(0,len(splits_list)):\n",
    "    kf = KFold(n_splits=splits_list[i]) \n",
    "\n",
    "    for j in range(0,20): \n",
    "        for train_index, test_index in kf.split(X):\n",
    "        \n",
    "            X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "            y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "            nn = MLPClassifier(alpha=0.001,activation = 'relu',random_state=1,solver='lbfgs',hidden_layer_sizes=(100,))\n",
    "            nn.fit(X_train,y_train)\n",
    "            y_pred = nn.predict(X_test)\n",
    "            nn_acc = accuracy_score(y_test,y_pred)\n",
    "            accuracy_list.append(nn_acc)\n",
    "            nn_pre = precision_score(y_test,y_pred)\n",
    "            precision_list.append(nn_pre)\n",
    "            nn_rec = recall_score(y_test,y_pred)\n",
    "            recall_list.append(nn_rec)\n",
    "            nn_f  =  f1_score(y_test,y_pred)\n",
    "            F_list.append(nn_f)\n",
    "\n",
    "            \n",
    "print(\"Average Accuracy = \",np.mean((np.array(accuracy_list))) )\n",
    "print(\"Standard Deviation Accuracy = \",np.std((np.array(accuracy_list))))\n",
    "\n",
    "print(\"Average Precision = \",np.mean((np.array(precision_list))) )\n",
    "print(\"Standard Deviation Precision = \",np.std((np.array(precision_list))))\n",
    "\n",
    "print(\"Average Recall = \",np.mean((np.array(recall_list))) )\n",
    "print(\"Standard Deviation Recall = \",np.std((np.array(recall_list))))\n",
    "\n",
    "print(\"Average F Score = \",np.mean((np.array(F_list))) )\n",
    "print(\"Standard Deviation F Score = \",np.std((np.array(F_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaination\n",
    "KNN:\n",
    "    Across all the classifiers KNN has the minimum training time because it doesnot require training at all because all the \n",
    "    classification is done during test where pair wise distances are calculated depending upon the number of neighbors. As a\n",
    "    result of this, KNN has the maximum testing time of about 0.207 seconds. It also offers a poor average accuracy of about 75 %.\n",
    "    The only positive is its minimal training time.\n",
    "SVM:\n",
    "    SVM has a decent but not the most optimal performance across all the six classifiers. It has an average accuracy of about 90 %\n",
    "    with a significantly less testing time of about 0.057 seconds as compared to KNN. However, its training time is significantly\n",
    "    higher than four out of the remaining six. Nonetheless, it is a parametric model with a reasonable high accuracy.\n",
    "Random Forest (Default Parameters):\n",
    "    Random Forest with default parameters has a significantly high accuracy of about 94 % and a low training and testing time of\n",
    "    0.129 seconds and 0.003 seconds respectively. It is an ensemble method therefore it performs a lot better than most of the\n",
    "    other classifiers present. However, its accuracy is still not optimal and can be improved upto an extent.\n",
    "Neural Networks (Default Parameters):\n",
    "    Neural Networks with default parameters have a daunting training time of about 2.65 seconds. Eventhough, the testing time of\n",
    "    0.004 seconds is less and accuracy of 90 % is decent but with a such a large training time, the classifier is obviously not\n",
    "    the perfect choice for the data set.\n",
    "Random Forest (Optimal Parameters):\n",
    "    Random Forests with optimal parameters is by far the best choice in terms of accuracy. It has an impressive average accuracy\n",
    "    of about 96%. In addition, it has a reasonable training time of 0.212 seconds and testing time of 0.005 seconds. This makes\n",
    "    it perhaps the best of the rest of the six classifiers.\n",
    "Neural Networks (Optimal Parameters):\n",
    "    Neural Networks with optimal parameters is a huge improvement from its default parameters version. It now has an increased \n",
    "    accuracy of 91 % and a significantly reduced (about 10 times) training time of 0.283 seconds and smallest testing time of \n",
    "    0.001 seconds. However,its training time is still very high as compared to the other five classifiers.\n",
    "Result:\n",
    "    According to the time and accuracy analysis, Random Forests with optimal parameters is the go to choice with the data set \n",
    "    at hand. It has the best accuracy beating the second highest with about 2 % and extremely low testing time and a more\n",
    "    than decent training time. It is no doubt the best pick out of all the six classifiers.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaination:\n",
    "We have repeated the classification 20 times to make sure that the entire volume of data is used as the training and test set\n",
    "respectively so that the models do not overfit to particular part of the data and generalize to the new inputs given in the \n",
    "future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Importances = ', [0.007331042336518689, 0.0068368122544985805, 0.007563641949898216, 0.008198187272377113, 0.0075200522077407085, 0.00708031217951373, 0.007195155292274915, 0.008848622168875274, 0.009012104590641584, 0.0075757327018675215, 0.007531604256753403, 0.009904065124597777, 0.008977004239565801, 0.008637127909998484, 0.013414559765739022, 0.010250986045963698, 0.01253725368718682, 0.013885834314140746, 0.014915120486398573, 0.013692447370236364, 0.014737757973785333, 0.016437654417548547, 0.010904229609143652, 0.01941914892970433, 0.016803984770771187, 0.007288342410537013, 0.03488847640783015, 0.13740414172828277, 0.12773907653218863, 0.0766217451742689, 0.08471711087393756, 0.026486991205435892, 0.031113346019253016, 0.02781643382652445, 0.012649392138739793, 0.007814936056262448, 0.006842647748738247, 0.00798065796560796, 0.00787726281818683, 0.007792267471480653, 0.007526919405026737, 0.008113291172795373, 0.006950697819236363, 0.007812276422475577, 0.007383898296603472, 0.007803204644832672, 0.007148034409661244, 0.007371399478209443, 0.0072332845705707, 0.007346107076473326, 0.007137668155835039, 0.007299559738595787, 0.007162264957729857, 0.007128412462093669, 0.007379785560603517, 0.007388869093407999, 0.00757105650283478])\n",
      "('Least Important Feature Number = ', '2')\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "forest.fit(X.values, y.values)\n",
    "importances = list(forest.feature_importances_)\n",
    "print(\"Importances = \",importances)\n",
    "unimportant = importances.index(min(importances))\n",
    "feature_names = list(data)\n",
    "print(\"Least Important Feature Number = \",feature_names[unimportant])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaination:\n",
    "We have removed feature number 2 because it has the minimal importance when compared with all the other features in the data.\n",
    "\n",
    "If we would have done classification on two features then these two features would have been the ones with the maximum importnace\n",
    "which are features 27 and 28 with a combined importance of about 25% of the entire data. This would have resulted in loss of\n",
    "some important information and would have resulted in less accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
